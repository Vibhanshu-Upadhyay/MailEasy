{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "0Ah-DnbP8uqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  bitsandbytes \\\n",
        "  unsloth \\\n",
        "  google-api-python-client \\\n",
        "  google-auth \\\n",
        "  google-auth-oauthlib"
      ],
      "metadata": {
        "id": "dvlLFesJH898"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mounting drive** (so don't have to download model everytime)"
      ],
      "metadata": {
        "id": "GWEdZDNcthm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "MODEL_DIR = \"/content/drive/MyDrive/hf_models/llama3_1_8b_4bit\""
      ],
      "metadata": {
        "id": "JdTWe7jptdGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()  # upload credentials.json"
      ],
      "metadata": {
        "id": "e3XULwTRH_qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google_auth_oauthlib.flow import InstalledAppFlow\n",
        "from google.oauth2.credentials import Credentials\n",
        "from googleapiclient.discovery import build\n",
        "from google.auth.transport.requests import Request\n",
        "\n",
        "SCOPES = [\"https://www.googleapis.com/auth/gmail.readonly\"]\n",
        "CREDENTIALS_FILE = \"credentials.json\"\n",
        "TOKEN_FILE = \"token.json\"\n",
        "\n",
        "def get_gmail_service_manual():\n",
        "    creds = None\n",
        "\n",
        "    if os.path.exists(TOKEN_FILE):\n",
        "        creds = Credentials.from_authorized_user_file(TOKEN_FILE, SCOPES)\n",
        "\n",
        "    if not creds or not creds.valid:\n",
        "        flow = InstalledAppFlow.from_client_secrets_file(\n",
        "            CREDENTIALS_FILE,\n",
        "            SCOPES,\n",
        "            redirect_uri=\"urn:ietf:wg:oauth:2.0:oob\"  # üîë critical\n",
        "        )\n",
        "\n",
        "        auth_url, _ = flow.authorization_url(\n",
        "            prompt=\"consent\",\n",
        "            access_type=\"offline\"\n",
        "        )\n",
        "\n",
        "        print(\"üîê Open this URL in your browser:\\n\")\n",
        "        print(auth_url)\n",
        "\n",
        "        # ‚¨áÔ∏è YOU PASTE THE CODE HERE\n",
        "        auth_code = input(\"\\nüìé Paste the authorization code here: \").strip()\n",
        "\n",
        "        flow.fetch_token(code=auth_code)\n",
        "        creds = flow.credentials\n",
        "\n",
        "        with open(TOKEN_FILE, \"w\") as f:\n",
        "            f.write(creds.to_json())\n",
        "\n",
        "    return build(\"gmail\", \"v1\", credentials=creds)\n"
      ],
      "metadata": {
        "id": "kl-AZ5WmIFX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service = get_gmail_service_manual()\n",
        "print(\"‚úÖ Gmail service ready\")"
      ],
      "metadata": {
        "id": "hWffAVr2NFvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fetch_emails.py\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import base64\n",
        "import os\n",
        "from google_auth_oauthlib.flow import InstalledAppFlow\n",
        "from google.oauth2.credentials import Credentials\n",
        "from googleapiclient.discovery import build\n",
        "from google.auth.transport.requests import Request\n",
        "\n",
        "\n",
        "SCOPES = [\"https://www.googleapis.com/auth/gmail.readonly\"]\n",
        "CREDENTIALS_FILE = \"credentials.json\"\n",
        "TOKEN_FILE = \"token.json\"\n",
        "\n",
        "def get_gmail_service():\n",
        "    creds = None\n",
        "\n",
        "    if os.path.exists(TOKEN_FILE):\n",
        "        creds = Credentials.from_authorized_user_file(\n",
        "            TOKEN_FILE, SCOPES\n",
        "        )\n",
        "\n",
        "    if not creds or not creds.valid:\n",
        "        if creds and creds.expired and creds.refresh_token:\n",
        "            creds.refresh(Request())\n",
        "        else:\n",
        "            raise RuntimeError(\n",
        "                \"token.json missing or invalid. Run manual OAuth once.\"\n",
        "            )\n",
        "\n",
        "    return build(\"gmail\", \"v1\", credentials=creds)\n",
        "\n",
        "def get_last_12h_timestamp():\n",
        "    return int((datetime.utcnow() - timedelta(hours=12)).timestamp())\n",
        "\n",
        "def extract_body(payload):\n",
        "    if \"parts\" in payload:\n",
        "        for part in payload[\"parts\"]:\n",
        "            if part[\"mimeType\"] == \"text/plain\":\n",
        "                data = part[\"body\"].get(\"data\")\n",
        "                if data:\n",
        "                    return base64.urlsafe_b64decode(data).decode(\"utf-8\", errors=\"ignore\")\n",
        "    data = payload[\"body\"].get(\"data\")\n",
        "    if data:\n",
        "        return base64.urlsafe_b64decode(data).decode(\"utf-8\", errors=\"ignore\")\n",
        "    return \"\"\n",
        "\n",
        "def normalize_message(msg, full_msg):\n",
        "    headers = {h[\"name\"]: h[\"value\"] for h in full_msg[\"payload\"][\"headers\"]}\n",
        "\n",
        "    return {\n",
        "        \"id\": msg[\"id\"],\n",
        "        \"thread_id\": msg[\"threadId\"],\n",
        "        \"from\": headers.get(\"From\", \"\"),\n",
        "        \"subject\": headers.get(\"Subject\", \"\"),\n",
        "        \"date\": headers.get(\"Date\", \"\"),\n",
        "        \"labels\": full_msg.get(\"labelIds\", []),\n",
        "        \"body_text\": extract_body(full_msg[\"payload\"]),\n",
        "    }\n",
        "\n",
        "def fetch_last_12h_emails():\n",
        "    service = get_gmail_service()\n",
        "    after_ts = get_last_12h_timestamp()\n",
        "\n",
        "    results = service.users().messages().list(\n",
        "        userId=\"me\",\n",
        "        q=f\"after:{after_ts}\"\n",
        "    ).execute()\n",
        "\n",
        "    messages = results.get(\"messages\", [])\n",
        "    emails = []\n",
        "\n",
        "    for msg in messages:\n",
        "        full_msg = service.users().messages().get(\n",
        "            userId=\"me\", id=msg[\"id\"], format=\"full\"\n",
        "        ).execute()\n",
        "\n",
        "        emails.append(normalize_message(msg, full_msg))\n",
        "\n",
        "    return emails\n"
      ],
      "metadata": {
        "id": "1BmRXKOzINHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fetch_emails import fetch_last_12h_emails\n",
        "\n",
        "print(fetch_last_12h_emails())"
      ],
      "metadata": {
        "id": "OxXAdxt3voma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VqrPaGKmrfN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "HF_MODEL = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    HF_MODEL,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"LLaMA model loaded.\")\n"
      ],
      "metadata": {
        "id": "Drxb7k5HKHTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llama_chat.py\n",
        "\n",
        "from model import model, tokenizer\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MAX_INPUT_TOKENS = 2048\n",
        "MAX_NEW_TOKENS = 200\n",
        "\n",
        "def llama_chat(system_prompt, user_prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_INPUT_TOKENS\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=False,\n",
        "            repetition_penalty=1.15,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    generated = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n"
      ],
      "metadata": {
        "id": "blMju3OqKKzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile intent_classifier.py\n",
        "\n",
        "from llama_chat import llama_chat\n",
        "\n",
        "PRIMARY_LABELS = [\"IGNORE\", \"READ_ONLY\", \"ACTION_REQUIRED\"]\n",
        "SPAM_LABELS = [\"IGNORE\", \"ATTENTION_REQUIRED\"]\n",
        "\n",
        "def build_email_text(mail):\n",
        "    return f\"\"\"From: {mail['from']}\n",
        "Subject: {mail['subject']}\n",
        "\n",
        "Body:\n",
        "{mail['body_text'][:1200]}\n",
        "\"\"\"\n",
        "\n",
        "def classify_intent(mail, mailbox):\n",
        "    prompt = (\n",
        "        \"Classify this email into IGNORE, READ_ONLY, ACTION_REQUIRED\"\n",
        "        if mailbox == \"PRIMARY\"\n",
        "        else \"Classify this email into IGNORE or ATTENTION_REQUIRED\"\n",
        "    )\n",
        "\n",
        "    response = llama_chat(\n",
        "        \"You are an email intent classifier. Return ONLY the label.\",\n",
        "        f\"{prompt}\\n\\n{build_email_text(mail)}\"\n",
        "    ).upper()\n",
        "\n",
        "    for label in (PRIMARY_LABELS if mailbox == \"PRIMARY\" else SPAM_LABELS):\n",
        "        if label in response:\n",
        "            return label\n",
        "\n",
        "    return \"READ_ONLY\"\n"
      ],
      "metadata": {
        "id": "WtOS22StKo8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile summarizer.py\n",
        "\n",
        "from llama_chat import llama_chat\n",
        "from intent_classifier import build_email_text\n",
        "\n",
        "def summarize_email(mail):\n",
        "    return llama_chat(\n",
        "    system_prompt=(\n",
        "        \"You summarize emails for a human user.\\n\\n\"\n",
        "        \"Guidelines:\\n\"\n",
        "        \"- The email body may contain HTML, images, tracking links, or very little readable text.\\n\"\n",
        "        \"- If meaningful text exists, summarize it accurately.\\n\"\n",
        "        \"- If the body is mostly HTML, images, or boilerplate:\\n\"\n",
        "        \"  - Infer the purpose from subject, sender, and visible text\\n\"\n",
        "        \"  - Make a reasonable, conservative summary\\n\"\n",
        "        \"- Do NOT say \\\"not enough information\\\"\\n\"\n",
        "        \"- Do NOT mention HTML, images, or missing data\\n\"\n",
        "        \"- Do NOT hallucinate specific facts or details\\n\\n\"\n",
        "        \"Use plain English.\\n\"\n",
        "        \"Be concise, helpful, and realistic.\"\n",
        "    ),\n",
        "    user_prompt=f\"\"\"Summarize this email in 3 to 4 bullet points.\n",
        "\n",
        "{build_email_text(mail)}\n",
        "\"\"\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "bOiQwfgbKsb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile draft_generator.py\n",
        "\n",
        "from llama_chat import llama_chat\n",
        "from intent_classifier import build_email_text\n",
        "\n",
        "def generate_draft(mail, intent, summary=None):\n",
        "    if intent not in {\"ACTION_REQUIRED\", \"READ_ONLY\", \"ATTENTION_REQUIRED\"}:\n",
        "        return None\n",
        "\n",
        "    context = build_email_text(mail)\n",
        "    if summary:\n",
        "        context += f\"\\n\\nSummary:\\n{summary}\"\n",
        "\n",
        "    return llama_chat(\n",
        "        \"You write professional email replies.\",\n",
        "        f\"{context}\\n\\nWrite a polite, professional reply.\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "cbjvYvTuKxpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails = fetch_last_12h_emails()\n",
        "\n",
        "for mail in emails:\n",
        "    mailbox = \"SPAM\" if \"SPAM\" in mail[\"labels\"] else \"PRIMARY\"\n",
        "\n",
        "    intent = classify_intent(mail, mailbox)\n",
        "    summary = summarize_email(mail)\n",
        "    draft = generate_draft(mail, intent, summary)\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"[{mailbox}] [{intent}] {mail['subject']}\")\n",
        "    print(\"From:\", mail[\"from\"])\n",
        "    print(\"\\nSUMMARY:\\n\", summary)\n",
        "\n",
        "    if draft:\n",
        "        print(\"\\nDRAFT:\\n\", draft)\n"
      ],
      "metadata": {
        "id": "rHL_Pd8BK1IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FRONTEND**"
      ],
      "metadata": {
        "id": "KW7gzyYovUXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**METHOD 2 ( STREAMLIT AND CLOUDFLARE)**"
      ],
      "metadata": {
        "id": "Um09CN0dxSWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit cloudflared"
      ],
      "metadata": {
        "id": "gU0SkpqcxXmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "from fetch_emails import fetch_last_12h_emails\n",
        "from intent_classifier import classify_intent\n",
        "from summarizer import summarize_email\n",
        "from draft_generator import generate_draft\n",
        "from llama_chat import llama_chat\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"MailEasy\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "st.title(\"üì¨ MailEasy ‚Äî AI Email Assistant\")\n",
        "st.markdown(\n",
        "    \"Fetch your last 12 hours of emails, understand intent, summarize, and draft replies using LLaMA.\"\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Main action button\n",
        "# -----------------------------\n",
        "if st.button(\"üöÄ Fetch & Process Emails\"):\n",
        "    with st.spinner(\"Fetching emails and running AI...\"):\n",
        "        emails = fetch_last_12h_emails()\n",
        "\n",
        "    if not emails:\n",
        "        st.warning(\"No emails found in the last 12 hours.\")\n",
        "    else:\n",
        "        st.success(f\"Processed {len(emails)} emails\")\n",
        "\n",
        "        for idx, mail in enumerate(emails, start=1):\n",
        "            mailbox = \"SPAM\" if \"SPAM\" in mail[\"labels\"] else \"PRIMARY\"\n",
        "\n",
        "            intent = classify_intent(mail, mailbox)\n",
        "            summary = summarize_email(mail)\n",
        "            draft = generate_draft(mail, intent, summary)\n",
        "\n",
        "            with st.expander(f\"üìß {idx}. {mail['subject']}\"):\n",
        "                st.markdown(f\"**From:** {mail['from']}\")\n",
        "                st.markdown(f\"**Mailbox:** `{mailbox}`\")\n",
        "                st.markdown(f\"**Intent:** `{intent}`\")\n",
        "\n",
        "                st.subheader(\"üìù Summary\")\n",
        "                st.write(summary or \"‚Äî\")\n",
        "\n",
        "                if draft:\n",
        "                    st.subheader(\"‚úâÔ∏è Draft Reply\")\n",
        "                    st.text_area(\n",
        "                        \"Draft\",\n",
        "                        draft,\n",
        "                        height=180,\n",
        "                        key=f\"draft_{idx}\"\n",
        "                    )\n",
        "                else:\n",
        "                    st.info(\"No reply required.\")\n"
      ],
      "metadata": {
        "id": "80QN0M0kxawK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 /usr/local/bin/cloudflared"
      ],
      "metadata": {
        "id": "Mo8PqGRS0g55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart Streamlit in the background, but let's monitor its output to ensure it starts without errors\n",
        "# This time, we will not redirect stderr to /dev/null, so you can see any potential startup errors.\n",
        "get_ipython().system_raw('streamlit run app.py --server.port 8501 --server.address 0.0.0.0 &')\n",
        "print(\"Streamlit app launched in the background. Check the output above for any errors. If no errors, proceed to restart cloudflared.\")"
      ],
      "metadata": {
        "id": "Ll1jCYpYxf31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cloudflared tunnel --url http://localhost:8501\n"
      ],
      "metadata": {
        "id": "Sl8M6OUEzSF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "566f5ccc"
      },
      "source": [
        "# Stop any existing streamlit processes to ensure a clean restart\n",
        "!pkill -f streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bdfeda3"
      },
      "source": [
        "# Restart Streamlit in the background, but let's monitor its output to ensure it starts without errors\n",
        "# This time, we will not redirect stderr to /dev/null, so you can see any potential startup errors.\n",
        "get_ipython().system_raw('streamlit run app.py --server.port 8501 --server.address 0.0.0.0 &')\n",
        "print(\"Streamlit app launched in the background. Check the output above for any errors. If no errors, proceed to restart cloudflared.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73b0ca90"
      },
      "source": [
        "# Restart cloudflared tunnel after ensuring streamlit is running\n",
        "!pkill -f cloudflared\n",
        "!cloudflared tunnel --url http://localhost:8501"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}